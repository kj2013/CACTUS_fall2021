{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 27 13:50:43 2019\n",
    "\n",
    "@author: Kokil\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "#!pip install spacy\n",
    "#!python3 -m spacy download en_core_web_lg \n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "#!pip install convokit\n",
    "from convokit import Corpus, Speaker, Utterance\n",
    "from convokit import download\n",
    "from convokit import TextParser\n",
    "from convokit import PolitenessStrategies\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def is_number(tok):\n",
    "    try:\n",
    "        float(tok)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    return [tok.text if not is_number(tok.text) else '_NUM_' for tok in nlp(text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_harbingers(df, X_col):\n",
    "\n",
    "    with open('data/2015_Diplomacy_lexicon.json') as f:\n",
    "        features = json.loads(f.readline())\n",
    "\n",
    "    for feature in features:\n",
    "        harbingers = [harbinger.encode('ascii', 'ignore').decode('ascii').lower() for harbinger in features[feature]]\n",
    "        features[feature] = harbingers\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = text.replace('\\'', '')\n",
    "        text = text.lower()\n",
    "        text = text.replace('{html}',\"\") \n",
    "        text = re.sub(re.compile('<.*?>'), '', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(text)  \n",
    "        text = \" \".join(tokens)\n",
    "        return text\n",
    "\n",
    "    def get_feature_frequency(text, feature):\n",
    "        count = 0\n",
    "        for harbinger in features[feature]:\n",
    "            count += text.count(harbinger)\n",
    "        return count\n",
    "\n",
    "    df['clean_text'] = df.apply(lambda row: clean_text(row[X_col]), axis=1)\n",
    "    for feature in features:\n",
    "        df[feature] = df.apply(lambda row: get_feature_frequency(row['clean_text'], feature), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nlp = en_core_web_sm.load()\n",
    "#spacy.load(\"en_core_web_lg\")\n",
    "#spacy.load(\"en_core_web_sm\")\n",
    "ps = PolitenessStrategies()\n",
    "spacy_nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "cols = list(ps.transform_utterance(\"hello, could you please help me proofread this article?\", spacy_nlp=spacy_nlp).meta['politeness_strategies'])\n",
    "\n",
    "def extract_politeness_feats(df, X_col):\n",
    "\n",
    "    def extract_politeness_helper(row):\n",
    "        utt = ps.transform_utterance(row[X_col], spacy_nlp=spacy_nlp)\n",
    "        feats = [utt.meta['politeness_strategies'][x] for x in cols]\n",
    "        return pd.Series(feats)\n",
    "\n",
    "    df[cols] = df.apply(extract_politeness_helper, axis=1)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List harbingers, liwc and politeness features\n",
    "import json\n",
    "with open('data/2015_Diplomacy_lexicon.json') as f:\n",
    "    harb_dict = json.loads(f.readline())\n",
    "#main_df = pd.read_csv('data/polite_harbinger_featnames.csv')\n",
    "#X_cols = list(main_df.columns) + list(harb_dict.keys())\n",
    "X_cols = list(harb_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_feats(df, X_col):\n",
    "    extract_harbingers(df, X_col)\n",
    "    extract_politeness_feats(df, X_col)\n",
    "    newdf = emolize_messages( df,1,0,\"all\")\n",
    "    return newdf\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "happierfuntokenizer_v3\n",
    "\n",
    "This code implements a basic, Twitter-aware tokenizer.\n",
    "\n",
    "A tokenizer is a function that splits a string of text into words. In\n",
    "Python terms, we map string and unicode objects into lists of unicode\n",
    "objects.\n",
    "\n",
    "There is not a single right way to do tokenizing. The best method\n",
    "depends on the application.  This tokenizer is designed to be flexible\n",
    "and this easy to adapt to new domains and tasks.  The basic logic is\n",
    "this:\n",
    "\n",
    "1. The tuple regex_strings defines a list of regular expression\n",
    "   strings.\n",
    "\n",
    "2. The regex_strings strings are put, in order, into a compiled\n",
    "   regular expression object called word_re.\n",
    "\n",
    "3. The tokenization is done by word_re.findall(s), where s is the\n",
    "   user-supplied string, inside the tokenize() method of the class\n",
    "   Tokenizer.\n",
    "\n",
    "4. When instantiating Tokenizer objects, there is a single option:\n",
    "   preserve_case.  By default, it is set to True. If it is set to\n",
    "   False, then the tokenizer will downcase everything except for\n",
    "   emoticons.\n",
    "\n",
    "The __main__ method illustrates by tokenizing a few examples.\n",
    "\n",
    "I've also included a Tokenizer method tokenize_random_tweet(). If the\n",
    "twitter library is installed (http://code.google.com/p/python-twitter/)\n",
    "and Twitter is cooperating, then it should tokenize a random\n",
    "English-language tweet.\n",
    "\"\"\"\n",
    "\n",
    "######################################################################\n",
    "\n",
    "import re\n",
    "import html.entities\n",
    "######################################################################\n",
    "# The following strings are components in the regular expression\n",
    "# that is used for tokenizing. It's important that phone_number\n",
    "# appears first in the final regex (since it can contain whitespace).\n",
    "# It also could matter that tags comes after emoticons, due to the\n",
    "# possibility of having text like\n",
    "#\n",
    "#     <:| and some text >:)\n",
    "#\n",
    "# Most imporatantly, the final element should always be last, since it\n",
    "# does a last ditch whitespace-based tokenization of whatever is left.\n",
    "\n",
    "# This particular element is used in a couple ways, so we define it\n",
    "# with a name:\n",
    "emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8>]                    # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpPxX/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8<]                    # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      <[/\\\\]?3                         # heart(added: has)\n",
    "      |\n",
    "      \\(?\\(?\\#?                   #left cheeck\n",
    "      [>\\-\\^\\*\\+o\\~]              #left eye\n",
    "      [\\_\\.\\|oO\\,]                #nose\n",
    "      [<\\-\\^\\*\\+o\\~]              #right eye\n",
    "      [\\#\\;]?\\)?\\)?               #right cheek\n",
    "    )\"\"\"\n",
    "\n",
    "# The components of the tokenizer:\n",
    "regex_strings = (\n",
    "    # Phone numbers:\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "      (?:            # (international)\n",
    "        \\+?[01]\n",
    "        [\\-\\s.]*\n",
    "      )?            \n",
    "      (?:            # (area code)\n",
    "        [\\(]?\n",
    "        \\d{3}\n",
    "        [\\-\\s.\\)]*\n",
    "      )?    \n",
    "      \\d{3}          # exchange\n",
    "      [\\-\\s.]*   \n",
    "      \\d{4}          # base\n",
    "    )\"\"\"\n",
    "    ,\n",
    "    # Emoticons:\n",
    "    emoticon_string\n",
    "    ,    \n",
    "    # http:\n",
    "    # Web Address:\n",
    "    r\"\"\"(?:(?:http[s]?\\:\\/\\/)?(?:[\\w\\_\\-]+\\.)+(?:com|net|gov|edu|info|org|ly|be|gl|co|gs|pr|me|cc|us|gd|nl|ws|am|im|fm|kr|to|jp|sg)(?:\\/[\\s\\b$])?)\"\"\"\n",
    "    ,\n",
    "    r\"\"\"(?:http[s]?\\:\\/\\/)\"\"\"   #need to capture it alone sometimes\n",
    "    ,\n",
    "    #command in parens:\n",
    "    r\"\"\"(?:\\[[\\w_]+\\])\"\"\"   #need to capture it alone sometimes\n",
    "    ,\n",
    "    # HTTP GET Info\n",
    "    r\"\"\"(?:\\/\\w+\\?(?:\\;?\\w+\\=\\w+)+)\"\"\"\n",
    "    ,\n",
    "    # HTML tags:\n",
    "    r\"\"\"(?:<[^>]+\\w=[^>]+>|<[^>]+\\s\\/>|<[^>\\s]+>?|<?[^<\\s]+>)\"\"\"\n",
    "    #r\"\"\"(?:<[^>]+\\w+[^>]+>|<[^>\\s]+>?|<?[^<\\s]+>)\"\"\"\n",
    "    ,\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
    "    ,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[\\w][\\w'\\-_]+[\\w])       # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "######################################################################\n",
    "# This is the core tokenizing regex:\n",
    "    \n",
    "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
    "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
    "\n",
    "# These are for regularizing HTML entities to Unicode:\n",
    "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
    "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
    "amp = \"&amp;\"\n",
    "\n",
    "hex_re = re.compile(r'\\\\x[0-9a-z]{1,4}')\n",
    "\n",
    "######################################################################\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, preserve_case=False, use_unicode=True):\n",
    "        self.preserve_case = preserve_case\n",
    "        self.use_unicode = use_unicode\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        \"\"\"\n",
    "        Argument: s -- any string or unicode object\n",
    "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
    "        \"\"\"        \n",
    "        # Try to ensure unicode:\n",
    "        if self.use_unicode:\n",
    "            try:\n",
    "                s = str(s)\n",
    "            except UnicodeDecodeError:\n",
    "                s = str(s).encode('string_escape')\n",
    "                s = str(s)\n",
    "        # Fix HTML character entitites:\n",
    "        s = self.__html2unicode(s)\n",
    "        s = self.__removeHex(s)\n",
    "        # Tokenize:\n",
    "        words = word_re.findall(s)\n",
    "        #print words #debug\n",
    "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
    "        if not self.preserve_case:            \n",
    "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
    "        \n",
    "        return words\n",
    "\n",
    "\n",
    "    def __html2unicode(self, s):\n",
    "        \"\"\"\n",
    "        Internal metod that seeks to replace all the HTML entities in\n",
    "        s with their corresponding unicode characters.\n",
    "        \"\"\"\n",
    "        # First the digits:\n",
    "        ents = set(html_entity_digit_re.findall(s))\n",
    "        if len(ents) > 0:\n",
    "            for ent in ents:\n",
    "                entnum = ent[2:-1]\n",
    "                try:\n",
    "                    entnum = int(entnum)\n",
    "                    s = s.replace(ent, chr(entnum))\t\n",
    "                except:\n",
    "                    pass\n",
    "        # Now the alpha versions:\n",
    "        ents = set(html_entity_alpha_re.findall(s))\n",
    "        ents = filter((lambda x : x != amp), ents)\n",
    "        for ent in ents:\n",
    "            entname = ent[1:-1]\n",
    "            try:            \n",
    "                s = s.replace(ent, chr(htmlentitydefs.name2codepoint[entname]))\n",
    "            except:\n",
    "                pass                    \n",
    "            s = s.replace(amp, \" and \")\n",
    "        return s\n",
    "\n",
    "    def __removeHex(self, s):\n",
    "        return hex_re.sub(' ', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import requests\n",
    "from io import StringIO\n",
    "def LeXmo(text,document,dictionary):\n",
    "\n",
    "    '''\n",
    "      Takes text and adds if to a dictionary with 10 Keys  for each of the 10 emotions in the NRC Emotion Lexicon,\n",
    "      each dictionay contains the value of the text in that emotions divided to the text word count\n",
    "      INPUT: string\n",
    "      OUTPUT: dictionary with the text and the value of 10 emotions\n",
    "      '''\n",
    "    reponse = \"\"\n",
    "    emodic = {'text': text}\n",
    "    \n",
    "    if(dictionary == \"lexmo\"):\n",
    "        response = requests.get('https://raw.github.com/dinbav/LeXmo/master/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
    "        nrc = StringIO(response.text)\n",
    "\n",
    "\n",
    "\n",
    "        emodic = {'text': text, 'anger': [], 'anticipation': [], 'disgust': [], 'fear': [], 'joy': [], 'negative': [],\n",
    "                  'positive': [], 'sadness': [], 'surprise': [], 'trust': []}\n",
    "\n",
    "        df = pd.read_csv(nrc,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "\n",
    "    if(dictionary == \"liwc\"):\n",
    "        \n",
    "          \n",
    "\n",
    "\n",
    "        emodic = {'text': text, 'PPRON': [],'BODY': [],'WE': [],'DEATH': [],'FOCUSFUTURE': [],'FEEL': [],'INTERROG': [],'NUMBER': [],'POSEMO': [],'NEGATE': [],'QUANT': [],'THEY': [],'AFFECT': [],'RELATIV': [],'HOME': [],'CONJ': [],'COGPROC': [],'SEXUAL': [],'AUXVERB': [],'SHEHE': [],'BIO': [],'DIFFER': [],'POWER': [],'NETSPEAK': [],'INFORMAL': [],'CAUSE': [],'FILLER': [],'INSIGHT': [],'LEISURE': [],'NEGEMO': [],'MOTION': [],'SEE': [],'FOCUSPAST': [],'ANGER': [],'ARTICLE': [],'NONFLU': [],'MALE': [],'WORK': [],'FRIEND': [],'FUNCTION': [],'RISK': [],'FAMILY': [],'SPACE': [],'I': [],'IPRON': [],'SOCIAL': [],'ASSENT': [],'DRIVES': [],'PERCEPT': [],'VERB': [],'HEAR': [],'FEMALE': [],'DISCREP': [],'YOU': [],'ADJ': [],'ACHIEVE': [],'RELIG': [],'TENTAT': [],'COMPARE': [],'ADVERB': [],'PRONOUN': [],'MONEY': [],'FOCUSPRESENT': [],'INGEST': [],'AFFILIATION': [],'SWEAR': [],'HEALTH': [],'SAD': [],'TIME': [],'REWARD': [],'ANX': [],'PREP': [],'CERTAIN': []}\n",
    "        \n",
    "\n",
    "        df = pd.read_csv('data/liwc2015.txt',\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "    \n",
    "    if(dictionary == \"delib\"):\n",
    "        #response = requests.get('data/dd_delib.txt')\n",
    "        #delib = StringIO(response.text)\n",
    "        \n",
    "\n",
    "\n",
    "        emodic = {'text': text, 'EMP_RES': [],'UNCIVIL_ABUSE': [],'CONSTRUCTIVENESS': [],'JUSTIFICATION': [],'RECIPROCITY': [],'JUST_EXT': [],'RELEVANCE': [],'JUST_INT': []}\n",
    "        \n",
    "\n",
    "        df = pd.read_csv('data/dd_delib.txt',\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "    if(dictionary == \"hate\"):\n",
    "        #response = requests.get('data/incivilities.txt')\n",
    "        #hate = StringIO(response.text)\n",
    "        \n",
    "\n",
    "\n",
    "        emodic = {'text': text, 'SWEAR': [],'UNCIV': [],'OFFEN': []}\n",
    "        \n",
    "\n",
    "        df = pd.read_csv('data/incivilities.txt',\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        \n",
    "    if(dictionary == \"all\"):\n",
    "        #response = requests.get('data/incivilities.txt')\n",
    "        #hate = StringIO(response.text)\n",
    "        \n",
    "        emodic = {'text': text,'anger': [], 'anticipation': [], 'disgust': [], 'fear': [], 'joy': [], 'negative': [],\n",
    "                  'positive': [], 'sadness': [], 'surprise': [], 'trust': [],'PPRON': [],'BODY': [],'WE': [],'DEATH': [],'FOCUSFUTURE': [],'FEEL': [],'INTERROG': [],'NUMBER': [],'POSEMO': [],'NEGATE': [],'QUANT': [],'THEY': [],'AFFECT': [],'RELATIV': [],'HOME': [],'CONJ': [],'COGPROC': [],'SEXUAL': [],'AUXVERB': [],'SHEHE': [],'BIO': [],'DIFFER': [],'POWER': [],'NETSPEAK': [],'INFORMAL': [],'CAUSE': [],'FILLER': [],'INSIGHT': [],'LEISURE': [],'NEGEMO': [],'MOTION': [],'SEE': [],'FOCUSPAST': [],'ANGER': [],'ARTICLE': [],'NONFLU': [],'MALE': [],'WORK': [],'FRIEND': [],'FUNCTION': [],'RISK': [],'FAMILY': [],'SPACE': [],'I': [],'IPRON': [],'SOCIAL': [],'ASSENT': [],'DRIVES': [],'PERCEPT': [],'VERB': [],'HEAR': [],'FEMALE': [],'DISCREP': [],'YOU': [],'ADJ': [],'ACHIEVE': [],'RELIG': [],'TENTAT': [],'COMPARE': [],'ADVERB': [],'PRONOUN': [],'MONEY': [],'FOCUSPRESENT': [],'INGEST': [],'AFFILIATION': [],'SWEAR': [],'HEALTH': [],'SAD': [],'TIME': [],'REWARD': [],'ANX': [],'PREP': [],'CERTAIN': [], 'EMP_RES': [],'UNCIVIL_ABUSE': [],'CONSTRUCTIVENESS': [],'JUSTIFICATION': [],'RECIPROCITY': [],'JUST_EXT': [],'RELEVANCE': [],'JUST_INT': [],'SWEAR': [],'UNCIV': [],'OFFEN': []}\n",
    "        \n",
    "        response = requests.get('https://raw.github.com/dinbav/LeXmo/master/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt')\n",
    "        nrc = StringIO(response.text)\n",
    "\n",
    "        df1 = pd.read_csv(nrc,\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "\n",
    "  \n",
    "\n",
    "        df2 = pd.read_csv('data/incivilities.txt',\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        df3 = pd.read_csv('data/liwc2015.txt',\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        df4 = pd.read_csv('data/dd_delib.txt',\n",
    "                            names=[\"word\", \"emotion\", \"association\"],\n",
    "                            sep=r'\\t', engine='python')\n",
    "        df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "        df = df.drop_duplicates(subset=['word', 'emotion'])        \n",
    "        \n",
    "    df.reset_index()\n",
    "    # Remove duplicate word-emotion pairs by keeping the first occurrence\n",
    "\n",
    "\n",
    "    # Pivot the table for the selected dictionary\n",
    "    emolex_words = df.pivot(index='word', columns='emotion', values='association').fillna(0).reset_index()\n",
    "\n",
    "    categories = emolex_words.columns.drop('word')\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    # Process each word in the document using a lambda function\n",
    "    df_list = list(map(lambda word: emolex_words[emolex_words.word == stemmer.stem(word.lower())], document))\n",
    "    \n",
    "    # Concatenate the results and reset index\n",
    "    df = pd.concat(df_list)\n",
    "    df.reset_index(drop=True)\n",
    "\n",
    "    word_count = len(document)\n",
    "    for category in categories:\n",
    "        emodic[category] = df[category].sum() / word_count\n",
    "\n",
    "    return emodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def tokenize_messages(filename,col_text,col_msgid):\n",
    "    with open(filename,encoding=\"utf-8\") as corpus:\n",
    "            reader = csv.reader(corpus)\n",
    "            rows_list = []\n",
    "            for row in reader:\n",
    "                message = row[col_text]\n",
    "                tokenizer = Tokenizer(preserve_case=True)\n",
    "                words = tokenizer.tokenize(message.lower())\n",
    "                #print(words)\n",
    "                totalGrams=0\n",
    "                freqs = dict()    \n",
    "                totalChars = 0\n",
    "                gram = '' \n",
    "                for n in range (1,4):\n",
    "                    for i in range(0,(len(words) - n)+1):\n",
    "                        totalGrams += 1\n",
    "                        gram = ' '.join(words[i:i+n])\n",
    "                        try:\n",
    "                            freqs[gram] = 1\n",
    "                        except:\n",
    "                            print(\"error\")\n",
    "                freqs[\"message_id\"]=row[col_msgid]\n",
    "                rows_list.append(freqs)\n",
    "            df = pd.DataFrame(rows_list) \n",
    "            df= df.replace(np.nan, 0)\n",
    "            #print(\"Writing tokenized messages to csv...\")\n",
    "            #timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "            #print timestr\n",
    "            #df.to_csv(\"tokenized_messages_\"+timestr+\".csv\")\n",
    "            return df\n",
    "            \n",
    "############################\n",
    "\n",
    "def emolize_messages(df, col_text, col_msgid, choice):\n",
    "    # Define a function that will be applied to each row\n",
    "    def process_row(row):\n",
    "        message = row[col_text]\n",
    "        print(message)\n",
    "        tokenizer = Tokenizer(preserve_case=True)\n",
    "        words = tokenizer.tokenize(message.lower())\n",
    "        return LeXmo(message.lower(), words, choice)\n",
    "\n",
    "    # Apply the function to each row\n",
    "    results = df.apply(lambda row: process_row(row), axis=1)\n",
    "    \n",
    "    # Convert the series of dictionaries into a DataFrame\n",
    "    results_df = pd.DataFrame(results.tolist())\n",
    "    \n",
    "    # Add the message ID column to the results DataFrame\n",
    "    df = pd.concat([df, results_df], axis=1)\n",
    "    return df\n",
    "    #return emolized_df\n",
    "            \n",
    "############################\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that just having the presence of a good guy with a gun would prevent any action by a bad guy with a gun.\n",
      "Good question! And thats why I think gun control laws do not deter crime; gun ownership deters crime!!\n",
      "The only defense for a nuclear attack is to send up our own nukes to intercept them. Same goes for guns.\n",
      "Guns aren't the problem, people are the problem. We need to stop punishing law abiding citizens who own guns because of the actions of a group of deranged people who shouldn't have them.\n",
      "A good gun with a gun would cause most bad guys with a gun not to use their guns.\n",
      "All the trategies we were talking about would still happen, even if Auto GUNS DIDNT EXIST. Same amount of people, same outcome.\n",
      "The only defense for a nuclear attack is to send up our own nukes to intercept them. Same goes for guns.\n",
      "All of these dangerous cases exist, only because guns exist in the first place. Remove them all and violence will drop dramatically.\n",
      "Well, that's obviously untrue. Even if you think automatic guns should be legal, they clearly make it easier to kill more people in a shorter amount of time.\n",
      "Having a gun does not lead to violence and gang, having a mentality that seeks to force your will on another DOES lead to gang and crime.\n",
      "All of these dangerous cases exist, only because guns exist in the first place. Remove them all and violence will drop dramatically.\n"
     ]
    }
   ],
   "source": [
    "#filename = \"data/sample.csv\"\n",
    "#df = extract_feats(df,filename,\"text\")\n",
    "filename = \"data/sample.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "newdf = extract_feats(df,\"text\")\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M\")\n",
    "newdf.iloc[1:].to_csv(filename+\"_allfeatures_\"+timestr+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
