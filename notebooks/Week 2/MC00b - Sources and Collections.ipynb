{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Media Cloud: Sources and Collections\n",
    "====================================\n",
    "\n",
    "At this point you should be ready to query Media Cloud for data. **This notebook demonstrates how to browse and download information about the media sources and collections within Media Cloud system**. This explores some of the API methods under the hood of our [Source Manager tool](https://sources.mediacloud.org), which is used to browse and administer sources and collections in our system.\n",
    "\n",
    "Media Cloud is a suite of web tools to support research into media coverage online. The underlying database has 1.5 billion stories (as of early 2020). Every open-web news story is connected to a `media` source. Sources are grouped together into collections (via `tags`). Our primary collections are [geography-based](https://sources.mediacloud.org/#/collections/country-and-state) (at the national and provider/state level).\n",
    "\n",
    "We regularly scrape RSS feeds from a small set of our sources (around 60k as of early 2020). We are slowly rolling out the ability to ingest stories from news stories via their sitemap files (the hard part is determining which URLs arenews story pages and what are not). Other stories are discovered and added in via spidering links or finding a share of a news URL on social media. We don't advise using our entire database because it is skewed towards the topics of investigations ourselves, and collaborating researchers, have done. You can mitigate this by using the afore-mentioned geographic collections.\n",
    "\n",
    "Our Python API exposes a few methods that are particularly helpful for looking at sources, their associated metadata, and collections: \n",
    "\n",
    "* `mediaList`: useful to search for media, or page through all the media in a collection ([documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2medialist))\n",
    "* `media`: all metadata data about one media source, by `media_id` ([documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2mediasingle))\n",
    "* `feedList`: page through any RSS feeds associated with a media source ([documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2feedslist))\n",
    "* `tagSet`: collecitons are grouped into `tag_sets` ([documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2tag_setssingle))\n",
    "* `tagList`: list all the collections in a `tag_set` ([documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2tag_setslist))\n",
    "* `tag`: information about a collection ([documentation](https://github.com/berkmancenter/mediacloud/blob/master/doc/api_2_0_spec/api_2_0_spec.md#apiv2tagssingle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab your API key from the environment variable and create a client for talking to Media Cloud\n",
    "import os, mediacloud.api\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import JSON\n",
    "load_dotenv()  # load config from .env file\n",
    "mc = mediacloud.api.MediaCloud(os.getenv('MC_API_KEY'))\n",
    "mediacloud.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for Media Sources\n",
    "\n",
    "You can search for specific media, or media matching a set of criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to find a media source based on its URL\n",
    "matching_sources = mc.mediaList(name_like='hindustantimes', sort='num_stories')\n",
    "JSON(matching_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you'll notice is that our sources are rather noisy. We are trying to move to a model where each told level domain is a media source (with a handful of exceptions), but we haven't finished that work yet. So for it can be useful to sort there results by how content content they produce (ie. `sort='num_stories'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Media by Metadata\n",
    "Media sources have lots and lots of `tags` one them. Tags are used to represent many things in Media Cloud. In this case two relevant uses are:\n",
    "* tags are used to cluster media sources in collections\n",
    "* tags are used to add metadata to media sources - these are helpfully parsed out for you by the API client in the `metadata` property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use metadata tags to find media published in India in English\n",
    "TAG_PUBLISHED_IN_INDIA = 9353533\n",
    "TAG_PUBLISHED_IN_MOSTLY_ENGLISH = 9361422\n",
    "# this `tags_id_X` syntax is a little hokey, but it what we built quickly\n",
    "# `tags_id_X` clauses are AND'ed together, while the array of values for each are OR'ed together\n",
    "indian_english_sources = mc.mediaList(tags_id_1=[TAG_PUBLISHED_IN_INDIA],\n",
    "                                      tags_id_2=[TAG_PUBLISHED_IN_MOSTLY_ENGLISH],\n",
    "                                      sort='num_stories')\n",
    "[m['url'] for m in indian_english_sources]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paging Through Media Lists & Saving Result CSVs\n",
    "But you probably want to page through results to see all the matching sources in our system. `mediaList` supports that thorugh the `last_media_id` param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page through a list of media list results\n",
    "def all_media_list(**kwargs):\n",
    "    last_media_id = None\n",
    "    more_results = True\n",
    "    matching_media = []\n",
    "    while more_results:\n",
    "        media_page = mc.mediaList(**kwargs, last_media_id=last_media_id)\n",
    "        print(\"  got a page of {} matching media\".format(len(media_page)))\n",
    "        if len(media_page) == 0:\n",
    "            more_results = False\n",
    "        else:\n",
    "            matching_media += media_page\n",
    "            last_media_id = media_page[-1]['media_id']\n",
    "    return matching_media\n",
    "all_indian_english_sources = all_media_list(tags_id_1=[TAG_PUBLISHED_IN_INDIA],\n",
    "                                            tags_id_2=[TAG_PUBLISHED_IN_MOSTLY_ENGLISH],\n",
    "                                            sort='num_stories',\n",
    "                                            rows=100)\n",
    "\"found {} matching sources total\".format(len(all_indian_english_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and you may want to save that as a CSV, like Source Manager lets you do online\n",
    "fieldnames = ['media_id', 'url', 'name',\n",
    "              'pub_country', 'pub_state', 'primary_language', 'subject_country', 'media_type',\n",
    "              'public_notes', 'stories_per_day', 'first_story']\n",
    "for m in all_indian_english_sources: # do some data prep to make it easy to output\n",
    "    m['pub_country'] = m['metadata']['pub_country']['tag'] if m['metadata']['pub_country'] else None\n",
    "    m['pub_state'] = m['metadata']['pub_state']['tag'] if m['metadata']['pub_state'] else None\n",
    "    m['primary_language'] = m['metadata']['language']['tag'] if m['metadata']['language'] else None\n",
    "    m['subject_country'] = m['metadata']['about_country']['tag'] if m['metadata']['about_country'] else None\n",
    "    m['media_type'] = m['metadata']['media_type']['tag'] if m['metadata']['media_type'] else None\n",
    "    m['stories_per_day'] = m['num_stories_90']\n",
    "    m['first_story'] = m['start_date']\n",
    "# and write a CSV\n",
    "import csv\n",
    "with open('media-list.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for s in all_indian_english_sources:\n",
    "        writer.writerow(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Media Source Feeds\n",
    "Media Sources are created manually, or automatically generated by our system when a story is ingested from a domain we have not seen before. In the latter situation, a placeholder inactive RSS feed is generated to maintain database consistency (this feed usually has _\"#spidered\"_ on the end of its URL). For the limited number of sources that we ingest from daily, we have manually and automatically created RSS feeds (see our [`feed_seeker` package](https://github.com/mitmedialab/feed_seeker)).\n",
    "```\n",
    "media source\n",
    "  ↳ feed\n",
    "    ↳ story\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn about the first result from above, which is our canonical one for the Hindustan Times\n",
    "hindustan_times = mc.media(matching_sources[0]['media_id'])\n",
    "JSON(hindustan_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the feeds associated with this media source\n",
    "hindistan_times_feeds = mc.feedList(media_id=hindustan_times['media_id'], rows=200)\n",
    "JSON(hindistan_times_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but only some of these are active ones that we ingest news from every day\n",
    "active_feeds = [f for f in hindistan_times_feeds if f['active']]\n",
    "\"{}/{} of the feeds are checked for new stories each day\".format(len(active_feeds), len(hindistan_times_feeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch all the feeds and dump to a csv\n",
    "def all_source_feeds(media_id):\n",
    "    more_feeds = True\n",
    "    last_feeds_id = None\n",
    "    all_feeds = []\n",
    "    while more_feeds:\n",
    "        feed_page = mc.feedList(media_id, last_feeds_id=last_feeds_id, rows=100)\n",
    "        print(\"  fetched a page of {} feeeds\".format(len(feed_page)))\n",
    "        if len(feed_page) == 0:\n",
    "            more_feeds = False\n",
    "        else:\n",
    "            all_feeds += feed_page\n",
    "            last_feeds_id = feed_page[-1]['feeds_id']\n",
    "    return all_feeds\n",
    "all_hindistan_times_feeds = all_source_feeds(media_id=hindustan_times['media_id'])\n",
    "# dump to CSV\n",
    "fieldnames = ['feeds_id', 'active', 'type', 'media_id', 'name', 'url']\n",
    "filename = 'media-{}-feeds.csv'.format(hindustan_times['media_id'])\n",
    "with open(filename, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
    "    writer.writeheader()\n",
    "    for s in all_hindistan_times_feeds:\n",
    "        writer.writerow(s)\n",
    "print(\"Wrote {} feeds to {}\".format(len(all_hindistan_times_feeds), filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collections\n",
    "Media Sources are grouped together into collections. These are implement internally as `tags`. Collections are grouped together as `tag_sets` for convenience of internal system operations. This can be confusing to navigate as humans. We have tons of collections, but the geographic ones the most useful place to start investigating things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the geographic collections were seeeded from http://www.abyznewslinks.com in 2018; since then we have cleaned and augmented them\n",
    "GEOGRAPHIC_COLLECTIONS_TAG_SET = 15765102\n",
    "mc.tagSet(GEOGRAPHIC_COLLECTIONS_TAG_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the collections in this tag set\n",
    "geographic_collections = mc.tagList(tag_sets_id=GEOGRAPHIC_COLLECTIONS_TAG_SET)\n",
    "[c['label'] for c in geographic_collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page through a list of all of the collections in this tag_set, using the `last_tags_id` parameter\n",
    "def all_tags_in_tag_set(tag_sets_id):\n",
    "    more_tags = True\n",
    "    last_tags_id = None\n",
    "    all_tags = []\n",
    "    while more_tags:\n",
    "        tag_page = mc.tagList(tag_sets_id=tag_sets_id, rows=500, last_tags_id=last_tags_id)\n",
    "        print(\"  got a page of {} tags\".format(len(tag_page)))\n",
    "        if len(tag_page) == 0:\n",
    "            more_tags = False\n",
    "        else:\n",
    "            all_tags += tag_page\n",
    "            last_tags_id = tag_page[-1]['tags_id']\n",
    "    return all_tags\n",
    "\n",
    "all_geographic_collections = all_tags_in_tag_set(GEOGRAPHIC_COLLECTIONS_TAG_SET)\n",
    "\"Found {} total geographic collections\".format(len(all_geographic_collections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this isn't encoded into a heirarchy, but there are some conventions here:\n",
    "# 1. each country's national and state/local collections start with the country name\n",
    "# 2. each country's province/state level collections include their alpha2 name\n",
    "# for instance, this finds all the collections related to Spain\n",
    "spain_collections = [c for c in all_geographic_collections if c['label'] and (c['label'].startswith('Spain') or c['tag'].startswith('geo_ES-'))]\n",
    "[c['label'] for c in spain_collections]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: Media Source Metadata Tags\n",
    "Here is a quick utility to let you generate a list of all the values possible for each type of media metadata (aka the `tags` in each media metadata `tag_set`). These constants are available in `mediacloud.tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = ['tags_id', 'label', 'tag', 'description']\n",
    "for metadata_tag_sets_id in mediacloud.tags.METADATA_TAG_SETS:\n",
    "    tag_set = mc.tagSet(metadata_tag_sets_id)\n",
    "    print(\"{}:\".format(tag_set['label']))\n",
    "    tags_in_set = all_tags_in_tag_set(metadata_tag_sets_id)\n",
    "    filename = 'metadata-{}-tags.csv'.format(tag_set['name'])\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        for s in tags_in_set:\n",
    "            writer.writerow(s)\n",
    "    print(\"  Wrote {} tags to {}\".format(len(tags_in_set), filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
